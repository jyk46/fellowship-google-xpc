%=========================================================================
% sec-background
%=========================================================================

\section{Previous Research}
\label{sec-background}

Several aspects of the XPC architecture are inspired by our previous work
on general-purpose graphics processing units (GPGPUs). Although GPGPUs
can achieve higher performance and energy-efficiency than traditional
multicore architectures, the realized potential is critically dependent
on the control and memory-access structure inherent to the application.
Applications with ample conventional data parallelism often excel on
GPGPUs, whereas those with amorphous data parallelism tend to suffer from
serialized execution and suboptimal memory coalescing.

In our paper published at ISCA 2013, we focused on further improving the
performance and energy efficiency of conventional data-parallel
applications by exploiting \emph{value structure}, a new form of
structure where values can be encoded as a compact function of the thread
index~\cite{kim-simt-vstruct-isca2013}. Three techniques were developed
to allow certain arithmetic, memory, and branch instructions with
operands exhibiting value structure to perform the computation once
instead of having each thread perform a separate computation. Using these
techniques achieved speedups of up to 1.7$\times$ and an increase in
energy efficiency of up to 1.3$\times$ on conventional data parallel
applications. Unfortunately, the benefit of this technique was less
apparent for applications with amorphous data parallelism.

In our paper published at MICRO 2014, we attempted to address the
difficulty of efficiently mapping amorphous data-parallel applications to
GPGPUs by utilizing \emph{hardware worklists} to mitigate the classic
challenges involved with double-buffered software worklists: high memory
contention and suboptimal load balancing~\cite{kim-hwwl-micro2014}. Our
technique uses per-lane worklist banks and adds new instructions for
enqueuing/dequeuing tasks to/from the hardware worklist. A hardware work
redistribution unit enables dynamic load balancing of fine-grain tasks
across the worklist banks, and a virtualization scheme enables spilling
tasks to memory. Using these techniques resulted in speedups of up to
2.4$\times$ in amorphous data parallel applications, some of which only
had marginal speedups in previous studies.

