Microprocessor design has reached a point where the efficacy of simply
adding more cores onto the chip has been saturated. Due to this and many
other technological challenges, computer architects are increasingly
turning toward new abstractions to take advantage of the growing
transistor density. Although a coarse-grain approach to heterogeneous
architectures such as Intel
Haswell~\cite{hammarlund-intel-haswell-ieeemicro2014}, AMD
Kabini~\cite{bouvier-amd-kabini-ieeemicro2014}, and NVIDIA Tegra
4~\cite{krewell-nvidia-tegra4-mpr2013} have shown promise, emerging
trends in applications are demanding a more fundamental paradigm shift in
computer architecture. For example, a ubiquitous type of amorphous data
parallelism that encapsulates many types of existing data parallelism can
dynamically modify the underlying data structure as well as generate most
of its work dynamically~\cite{pingali-tao-pldi2011}. This yields
workloads that are unpredictable and very difficult to map to traditional
data-parallel accelerators. In order to overcome these challenges, I
propose a fine-grain heterogeneous architecture called the
Explicit-Parallel-Call (XPC) architecture. Several aspects of the XPC
architecture are inspired by my previous work on eliminating
redundancy~\cite{kim-simt-vstruct-isca2013} and hardware support for
storing and distributing tasks~\cite{kim-hwwl-micro2014} on GPGPUs. The
vision for the XPC architecture is to tightly integrate many lightweight
compute units (i.e., tiles) specialized for exploiting different types of
amorphous data parallelism. The goal is to explore both software and
hardware approaches for exposing, scheduling, and executing fine-grain
amorphous data parallel tasks on the XPC architecture.

To expose opportunities for fine-grain parallel tasks, a productive
parallel programming API and special XPC ISA will be provided. The
programming API offers multiple parallel primitives to make encoding
amorphous data parallelism easier. For example, primitives such
as \TT{speculative\_for} allow for built-in conflict resolution for
workloads with tasks that need to modify the underlying data
structure concurrently. These primitives can be mapped to the XPC ISA
which exposes fine-grain parallel tasks as parallel function calls. A
special \TT{pcall} instruction creates multiple instances of a function
that is eligible for task stealing by other tiles in the system. Note
that this does not mean the function will be executed in
parallel. Whether or not it is depends on the availability of other
threads as well as which XPC tile the code is running on.

To schedule fine-grain parallel tasks, an adaptive runtime is required to
dynamically match tasks with tiles optimal for the exhibited type of
parallelism. The runtime uses a cactus
stack~\cite{frigo-hyperobjects-spaa2009} to manage multiple views of the
software stack corresponding to tasks generated at different levels of
the call tree. For instance, tasks generated by the \TT{pcall}
instruction would be separate entries in the cactus stack. These tasks
can be stolen by other tiles based on heuristics collected by the runtime
that reflect how well suited the task is for the given tile. The processes
of generating and stealing tasks are normally handled through memory, but
the XPC architecture can use hardware acceleration to streamline these
processes. One technique is to employ a per-tile \emph{task cache} that
exploits intra-tile task locality by storing generated tasks in a
hardware cache to avoid memory accesses. This could be coupled with
a \emph{task distribution network} that connects the task caches to
facilitate rapid distribution of tasks to tiles attempting to steal
tasks.

To execute fine-grain parallel tasks, tiles specialized for varying
levels of amorphous data parallelism can be used. The Tightly-Coupled
Lanes (TCL) tile is a
highly-specialized accelerator for regular amorphous data
parallelism. Instruction fetch and decode are amortized by the control
processor then a group of threads are scheduled across tightly-coupled
computation lanes to execute in lock-step. The TCL tile struggles with
the serialization caused by control and memory-access irregularity in
workloads with more irregular amorphous data parallelism.
The Loosely-Coupled Lanes (LCL) tile is another accelerator for slightly more
irregular amorphous data parallelism. The key difference between the TCL
and LCL is that the LCL allows for decoupling in control flow between
lanes without significant overheads. This is achieved by using per-lane
instruction buffers that can contain different sections of code for each
lane to execute. LCL acts as a reasonable middle-ground between the TCL
and CMC tiles (discussed below) because although it has a higher
tolerance for control irregularity, LCL is still better suited for
simpler tasks on the lanes and can suffer from high memory-access
irregularity. The Cooperative Multicore (CMC) tile is a general-purpose
multicore processor that can handle highly irregular amorphous data
parallel applications. A grid of discrete cores are connected by a mesh
network to facilitate inter-core communication. CMC is the least energy
efficient tile of the three XPC tiles for applications which do not
exhibit highly irregular amorphous data parallelism. CMC could function
as a default tile for initially collecting heuristics to determine if a
specialized tile would be more suitable, in addition to being the optimal
tile for highly irregular amorphous data parallel applications which
cannot fully utilize the amortization offered by the TCL or LCL.

